-------------------------------------- Setting up projct structure -------------------------------------

1. Create repo, clone it in local
2. Create a virtual environment name "atlas" - conda create -n atlas
3. Activate the virtual environment
4. pip install cookiecutter
5. cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
6. Rename src.models -> src.model
7. git add - commit - push  

------------------------------------ Setup MLFlow on Dagshub -------------------------------------------------------

8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking uri and code snippet. (Also try: Go to MLFlow UI)
11. pip install dagshub mlflow

12. Run the experiment notebooks
13. git add - commit - push

14. dvc init
15. create a local folder as : "local_s3" (temporary work)
16. on terminal: "dvc remote add -d mylocal local_s3"

** Create your IAM user and S3 bucket on AWS. Save the credentials of IAM user as environment variables. ***

17. Add code to the below files/folders inside src dir:
    - logger
    - data_ingestion.py
    - data_preprocessing.py
    - feature_engineering.py
    - model_building.py
    - model_evaluation.py
    - register_model.py
